{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1875ef9f-dee3-4362-9f84-06e87660d095",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Invoice / Not Invoice Document Classifier\n",
    "***\n",
    "### Problem Statement\n",
    "1. Generate a solution to identify if a document is an invoice\n",
    "2. Categorize documents that are invoices or not invoices\n",
    "\n",
    "### Dataset\n",
    "* Labels &rarr; documents, including invoices\n",
    "* Important features &rarr; text content (optionally: layout)\n",
    "\n",
    "### Solution\n",
    "* Combination of OCR (Optical Character Recognition) and a machine learning model (CNN) for image classification\n",
    "    * OCR &rarr; extract text and layout information\n",
    "    * ML model &rarr; classify based on extracted features\n",
    "    * TensorFlow &rarr; CNN architecture\n",
    "\n",
    "### Model Evaluation\n",
    "* Split the dataset into sets &rarr; Training, Validation, Test\n",
    "* Metrics &rarr; Accuracy, Precision, Recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c92d6a-1f05-4609-82a1-2d90c094b582",
   "metadata": {},
   "source": [
    "### Workflow Steps\n",
    "| Step                | Description                                               | Tool       |\n",
    "|---------------------|-----------------------------------------------------------|:-----------|\n",
    "| Pre-Process         | Use OCR to extract text and features from documents       | PyPDF      |\n",
    "| Feature Engineering | Create features based on text and layout                  | PyPDF      |\n",
    "| Data Split          | Split data into sets: Training, Testing, Validation       | 70/15/15   |\n",
    "| Model Training      | Train the model on labeled data                           | TensorFlow |\n",
    "| Validation          | Tune model parameters based on validation set performance | TensorFlow |\n",
    "| Testing             | Evaluate the model using test data                        | TensorFlow |\n",
    "| Deployment          | Integrate into document processing pipeline               | TensorFlow |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c196c10-a2eb-428e-aec9-39e85fbb7340",
   "metadata": {},
   "source": [
    "### Machine Learning Model Methods\n",
    "| Step                   | Method                             |\n",
    "|------------------------|------------------------------------|\n",
    "| Model Method           | Convolutional Neural Network (CNN) |\n",
    "| Feature Engineering    | CNN (handles feature engineering)  |\n",
    "| Optimizer              | Adam                               |\n",
    "| Learning Rate          | 0.001                              |\n",
    "| Betas                  | Default                            |\n",
    "| Loss Function          | MeanSquaredError                   |\n",
    "| Evaluation Metrics     | RootMeanSquaredError               |\n",
    "| Hyperparameter Tuning  | BayesianOptimization               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b304248-8176-4063-b2c6-9749d52a1254",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750378ae-40e5-450c-9dba-ad87ad788651",
   "metadata": {},
   "source": [
    "## Extract, Load, Review Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82f09284-659a-4704-a24c-d61aae48ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d723de-657d-46a6-944a-6ccf035c1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Extract Images\n",
    "\n",
    "with tarfile.open('images.tar.gz', 'r:gz') as f:\n",
    "  f.extractall('images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c26424-4be1-4ae6-9a0a-f751aea49b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Review Images\n",
    "\n",
    "def show_imgs(image_files):\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        img = Image.open(img_path)   \n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "# show_imgs(image_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f67d7ce-bb56-4dd9-8b8f-5969473752e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Review Image Dimensions\n",
    "\n",
    "def show_img_dims(image_files):\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        with Image.open(img_path) as img:\n",
    "            img_size = img.size\n",
    "        print(f\"{img_file}: {img_size}\")\n",
    "    return\n",
    "\n",
    "# show_img_dims(image_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995bbbe-ad97-4508-ae72-d253c5b8d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "image_folder = 'images/images'\n",
    "image_files = os.listdir(image_folder)\n",
    "\n",
    "##-- Rescale/standardize images\n",
    "def rescale_imgs(image_files, standard_size=(224, 224), new_folder=\"images_rescaled\"):\n",
    "\n",
    "     # Create new directory\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "        \n",
    "    rescaled_images={}\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        with Image.open(img_path) as img:\n",
    "            img_rescaled = img.resize(standard_size).convert(\"RGB\")\n",
    "            rescaled_images[img_file] = img_rescaled\n",
    "            save_path = os.path.join(new_folder, img_file)\n",
    "            img_rescaled.save(save_path)\n",
    "\n",
    "            \n",
    "##-- Get list of rescaled image paths\n",
    "rescaled_image_folder = 'images_rescaled'\n",
    "rescaled_image_files = [os.path.join(rescaled_image_folder, fname) for fname in os.listdir(rescaled_image_folder)]\n",
    "\n",
    "##-- Invoice categorization based on file name or text contents from file\n",
    "def categorize_inv(filename):\n",
    "    match = re.search(r'_([a-zA-Z]+).jpg$', filename)\n",
    "    if match:\n",
    "        try:\n",
    "            invoice = match.group(1).char.isalpha().lower()==(\"invoice\")\n",
    "            return invoice\n",
    "        except:\n",
    "            with open(filename, 'rb') as f:\n",
    "                pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "                text = \"\"\n",
    "                for page_num in range(pdf_reader.numPages):\n",
    "                    page = pdf_reader.getPage(page_num)\n",
    "                    text += page.extractText()\n",
    "                invoice = \"invoice\" in text.lower()\n",
    "                return invoice\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "##-- Categorize Invoices and assign labels\n",
    "inv_labels = [categorize_inv(os.path.basename(file)) for file in rescaled_image_files if categorize_inv(os.path.basename(file)) is not None]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((rescaled_image_files, inv_labels))\n",
    "\n",
    "def load_and_preprocess_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # normalize to [0,1] range\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(load_and_preprocess_image)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=len(rescaled_image_files))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "##-- Count samples & calculate splits\n",
    "total = len(dataset)\n",
    "train_size,test_size = int(0.7 * total),int(0.15 * total)\n",
    "val_size = total - train_size - test_size\n",
    "\n",
    "##-- Split & batch dataset\n",
    "train_data = dataset.take(train_size).batch(batch_size)\n",
    "test_data = dataset.skip(train_size).take(test_size).batch(batch_size)\n",
    "val_data = dataset.skip(train_size + test_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b68170-858d-4c40-98dc-288d59f76365",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb36c1-bb11-4776-9828-bf38849ed2b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Selection, Training, Testing, Validation, & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79834d-8cd0-4245-b2e5-75ca7ee3816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Define CNN architecture & initialize sequential model\n",
    "def CNN_model(dropout_rate):\n",
    "\n",
    "    model = Sequential([\n",
    "        ##-- 1st Convolution Layer: 32 filters of size (3,3) w/ ReLU activation\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "\n",
    "        ##-- 1st Max Pooling Layer\n",
    "        ##-- Pooling size (2,2)\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        ##-- 2nd Convolution Layer\n",
    "        ##-- 64 filters of size (3,3) w/ ReLU activation\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "\n",
    "        ##-- 2nd Max Pooling Layer\n",
    "        ##-- Pooling size (2,2)\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        ##-- 3rd Convolution Layer\n",
    "        ##-- 128 filters of size (3,3) w/ ReLU activation\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "\n",
    "        ##-- 3rd Max Pooling Layer\n",
    "        ##-- Pooling size (2,2)\n",
    "        MaxPooling2D(2, 2),\n",
    "\n",
    "        ##-- Flatten Layer\n",
    "        ##-- Flatten 3D output to 1D tensor\n",
    "        Flatten(),\n",
    "\n",
    "        ##-- Fully Connected Layer\n",
    "        ##-- 512 neurons with ReLU activation\n",
    "        Dense(512, activation='relu'),\n",
    "\n",
    "        ##-- Dropout Layer\n",
    "        ##-- Drop to avoid overfitting\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        ##-- Output Layer\n",
    "        ##-- 1 neuron with linear activation for regression\n",
    "        Dense(1, activation='linear')  \n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4c05928-9f62-446e-b14a-1573a49ade39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | dropou... |\n",
      "-------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m14.03    \u001b[0m | \u001b[0m0.3251   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m20.18    \u001b[0m | \u001b[95m0.4161   \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m23.66    \u001b[0m | \u001b[95m0.2      \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m35.43    \u001b[0m | \u001b[95m0.2907   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m14.78    \u001b[0m | \u001b[0m0.244    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m18.96    \u001b[0m | \u001b[0m0.2277   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m14.59    \u001b[0m | \u001b[0m0.2559   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m14.75    \u001b[0m | \u001b[0m0.3037   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m35.05    \u001b[0m | \u001b[0m0.319    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m26.72    \u001b[0m | \u001b[0m0.3616   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m18.59    \u001b[0m | \u001b[0m0.2907   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m20.61    \u001b[0m | \u001b[0m0.319    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m21.96    \u001b[0m | \u001b[0m0.2907   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m35.33    \u001b[0m | \u001b[0m0.319    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m17.65    \u001b[0m | \u001b[0m0.319    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m25.16    \u001b[0m | \u001b[0m0.3617   \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m16.72    \u001b[0m | \u001b[0m0.2      \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m23.14    \u001b[0m | \u001b[0m0.3616   \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m15.92    \u001b[0m | \u001b[0m0.3617   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m18.8     \u001b[0m | \u001b[0m0.3616   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m26.91    \u001b[0m | \u001b[0m0.2001   \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m14.0     \u001b[0m | \u001b[0m0.2001   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m15.56    \u001b[0m | \u001b[0m0.4066   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m32.61    \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[95m25       \u001b[0m | \u001b[95m35.79    \u001b[0m | \u001b[95m0.3881   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m15.61    \u001b[0m | \u001b[0m0.3881   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m24.23    \u001b[0m | \u001b[0m0.3881   \u001b[0m |\n",
      "| \u001b[95m28       \u001b[0m | \u001b[95m36.26    \u001b[0m | \u001b[95m0.3882   \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m19.59    \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m34.38    \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m34.71    \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m32.02    \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m18.5     \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m22.76    \u001b[0m | \u001b[0m0.3882   \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m20.99    \u001b[0m | \u001b[0m0.4384   \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m14.79    \u001b[0m | \u001b[0m0.4416   \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m19.42    \u001b[0m | \u001b[0m0.3693   \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m24.67    \u001b[0m | \u001b[0m0.3521   \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m34.76    \u001b[0m | \u001b[0m0.352    \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m20.5     \u001b[0m | \u001b[0m0.352    \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m35.26    \u001b[0m | \u001b[0m0.352    \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m16.19    \u001b[0m | \u001b[0m0.352    \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m24.4     \u001b[0m | \u001b[0m0.352    \u001b[0m |\n",
      "| \u001b[95m44       \u001b[0m | \u001b[95m36.74    \u001b[0m | \u001b[95m0.3521   \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m20.31    \u001b[0m | \u001b[0m0.3521   \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m17.67    \u001b[0m | \u001b[0m0.4037   \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m19.06    \u001b[0m | \u001b[0m0.3521   \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m25.14    \u001b[0m | \u001b[0m0.3521   \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m35.49    \u001b[0m | \u001b[0m0.3863   \u001b[0m |\n",
      "| \u001b[95m50       \u001b[0m | \u001b[95m37.35    \u001b[0m | \u001b[95m0.3521   \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m20.73    \u001b[0m | \u001b[0m0.3863   \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m23.27    \u001b[0m | \u001b[0m0.3862   \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m35.75    \u001b[0m | \u001b[0m0.3863   \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m23.91    \u001b[0m | \u001b[0m0.3072   \u001b[0m |\n",
      "| \u001b[95m55       \u001b[0m | \u001b[95m39.25    \u001b[0m | \u001b[95m0.3374   \u001b[0m |\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m35.19    \u001b[0m | \u001b[0m0.3374   \u001b[0m |\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m38.27    \u001b[0m | \u001b[0m0.3373   \u001b[0m |\n",
      "| \u001b[0m58       \u001b[0m | \u001b[0m18.58    \u001b[0m | \u001b[0m0.3373   \u001b[0m |\n",
      "| \u001b[0m59       \u001b[0m | \u001b[0m24.15    \u001b[0m | \u001b[0m0.3373   \u001b[0m |\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m16.81    \u001b[0m | \u001b[0m0.3375   \u001b[0m |\n",
      "=====================================\n",
      "{'target': 39.24965286254883, 'params': {'dropout_rate': 0.33735547894030193}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def train_model(dropout_rate):\n",
    "    model = CNN_model(dropout_rate)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    ## model.summary() ##--Summarize model\n",
    "    history = model.fit(train_data, epochs=10, verbose=0, validation_data=val_data)\n",
    "    val_rmse = history.history['val_root_mean_squared_error'][-1]\n",
    "    return val_rmse\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout_rate': (0.2, 0.5)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=50)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca6b8ad-a363-4f79-aae9-c6f8c0906b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Train model using optimal dropout rate\n",
    "best_dropout_rate = optimizer.max['params']['dropout_rate']\n",
    "final_model = CNN_model(best_dropout_rate)\n",
    "final_model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "final_model.fit(train_data, epochs=10, verbose=1, validation_data=val_data)\n",
    "\n",
    "##-- Test model to make predictions\n",
    "def make_prediction(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # normalize to [0,1] range\n",
    "    image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    inv_prediction = final_model.predict(image) \n",
    "    return inv_prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ae175-cff4-43ab-acaf-9ae4ac6f81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- EXAMPLE: Make a prediction for a new image\n",
    "# print(f\"Invoice Prediction: {make_prediction('images_rescaled/new_image.jpg')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03a43e-6dc7-4bcb-bb02-f3b3b7ecb982",
   "metadata": {},
   "source": [
    "## Model Monitoring & _pro re nata_ Retraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e7254-3c81-4922-81fb-825879f05451",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Initialize variables\n",
    "prev_test_rmse = None\n",
    "model = None\n",
    "\n",
    "##-- Set initial drift threshold\n",
    "drift_threshold = 0.1  \n",
    "\n",
    "##-- Model drift check: Compares current RMSE to previous RMSE\n",
    "def check_model_drift(current_test_rmse, prev_test_rmse):\n",
    "    if prev_test_rmse is None:\n",
    "        return False\n",
    "    \n",
    "    drift_ratio = abs(current_test_rmse - prev_test_rmse) / prev_test_rmse\n",
    "    return drift_ratio >= drift_threshold\n",
    "\n",
    "##-- Data drift check: Compares statistics of new data to old data\n",
    "def check_data_drift(new_data_stats, old_data_stats, threshold=0.05):\n",
    "    # Calculate drift based on statistics\n",
    "    drift = np.abs(new_data_stats - old_data_stats) > threshold\n",
    "    return np.any(drift)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ##-- Train model\n",
    "    if model is None:\n",
    "        \n",
    "        best_params = optimizer.max['params']\n",
    "        model = CNN_model(best_params['dropout_rate'])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "        model.fit(train_data, epochs=10, verbose=0, validation_data=val_data)\n",
    "\n",
    "    ##-- Test for model drift\n",
    "    test_predictions = model.predict(test_data)\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_predictions, test_data))\n",
    "    \n",
    "    if check_model_drift(test_rmse, prev_test_rmse):\n",
    "        print(\"Model drift detected.\")\n",
    "        model = None  # Set model to None to retrain next iteration\n",
    "    prev_test_rmse = test_rmse\n",
    "\n",
    "    ##-- Test for data drift\n",
    "    new_data_stats = np.mean([data for data, _ in train_data], axis=0)\n",
    "    if check_data_drift(new_data_stats, old_data_stats):\n",
    "        print(\"Data drift detected.\")\n",
    "        model = None  # Set model to None to retrain next iteration\n",
    "    old_data_stats = new_data_stats\n",
    "\n",
    "    ##-- Optionally: Add termination condition OR integrate scheduled checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6f9d2-a802-4802-8c71-3900db8cd532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
